{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13368952,"sourceType":"datasetVersion","datasetId":8481043},{"sourceId":13362630,"sourceType":"datasetVersion","datasetId":8476111}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Setup and Unzip All Images (CORRECTED)\nimport os\nimport shutil\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nprint(\"Step 1 shuru: Sabhi zip files se images ko nikal kar ek jagah jama kiya ja raha hai...\")\n\n# --- Configuration (UPDATED PATH) ---\n# Original data ka path\nDATA_DIR = \"/kaggle/input/ml-am-ds/student_resource/dataset/\" \n# Sahi path jahan aapki zip files hain (most likely 'zipped_batches' subfolder mein)\nIMAGES_DATASET_DIR = \"/kaggle/input/amazon-product-images/\" \n# Final folder jahan saari images aayengi\nFINAL_IMG_DIR = \"/kaggle/working/all_images/\" \n\nos.makedirs(FINAL_IMG_DIR, exist_ok=True)\n\n# --- Unzipping Logic ---\ndef unzip_all_batches(source_dir, dest_dir):\n    # source_dir ke andar maujood sabhi .zip files ko dhoondhein\n    zip_files = glob.glob(os.path.join(source_dir, \"*.zip\"))\n    print(f\"'{source_dir}' se {len(zip_files)} zip files mili.\")\n    \n    # Agar 0 files mili to error dega.\n    if len(zip_files) == 0:\n        print(\"\\nüö®üö® CRITICAL ERROR: Koi zip file nahi mili. Path dobara check karein! üö®üö®\")\n        print(\"Agar aapki files direct '/kaggle/input/amazon-product-images/' mein hain, to upar wala path badal dein.\")\n        return 0\n        \n    for zip_file in tqdm(zip_files, desc=\"Unzipping batches\"):\n        try:\n            shutil.unpack_archive(zip_file, dest_dir)\n        except Exception as e:\n            print(f\"Error unzipping {zip_file}: {e}\")\n            \n    return len(os.listdir(dest_dir))\n\n# Images ko final directory mein unzip karein\ntotal_images = unzip_all_batches(IMAGES_DATASET_DIR, FINAL_IMG_DIR)\n\n# --- CLEANUP LINES REMOVED: '/kaggle/input' se kuch bhi delete nahi kiya ja sakta. ---\n# shutil.rmtree(os.path.join(\"/kaggle/input\", \"amazon-product-images\")) line hata di gayi hai.\n\nif total_images > 0:\n    print(f\"\\n‚úÖ Step 1 poora hua! Kul {total_images} images '{FINAL_IMG_DIR}' folder mein hain.\")\n    print(\"Ab aap bina kisi badlav ke STEP 2 aur STEP 3 run kar sakte hain.\")\nelse:\n    print(\"\\n‚ùå Step 1 FAILED. Please check your Kaggle data path.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:53:06.364327Z","iopub.execute_input":"2025-10-13T13:53:06.364989Z","iopub.status.idle":"2025-10-13T13:53:06.624129Z","shell.execute_reply.started":"2025-10-13T13:53:06.364965Z","shell.execute_reply":"2025-10-13T13:53:06.623594Z"}},"outputs":[{"name":"stdout","text":"Step 1 shuru: Sabhi zip files se images ko nikal kar ek jagah jama kiya ja raha hai...\n'/kaggle/input/amazon-product-images/zipped_batches/' se 0 zip files mili.\n\nüö®üö® CRITICAL ERROR: Koi zip file nahi mili. Path dobara check karein! üö®üö®\nAgar aapki files direct '/kaggle/input/amazon-product-images/' mein hain, to upar wala path badal dein.\n\n‚ùå Step 1 FAILED. Please check your Kaggle data path.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 1/2 Combined: Direct Image Loading and Feature Extraction\n\nimport os\nimport shutil\nimport glob\nimport numpy as np\nimport torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nfrom tqdm import tqdm\n\nprint(\"üöÄ Direct Image Loading aur Feature Extraction shuru...\")\n\n# --- Configuration ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Sahi path jahan aapke 'images_batch_X' folders hain\nIMAGES_ROOT_DIR = \"/kaggle/input/amazon-product-images/\" \nEMBEDDINGS_FILE = \"/kaggle/working/image_embeddings.npz\"\n\nif DEVICE.type == 'cpu':\n    print(\"üö® WARNING: GPU not found. Please enable GPU in Notebook Settings.\")\n\n# --- Pre-trained Model (ResNet50) ko load karein ---\nmodel = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) \nmodel = torch.nn.Sequential(*list(model.children())[:-1]) \nmodel.to(DEVICE)\nmodel.eval()\n\n# --- Image Transformations ---\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# --- Feature nikaalne ka process ---\nimage_embeddings = {}\n# Sahi tarah se 'images_batch_X' folders ki list taiyar karein\nbatch_folders = glob.glob(os.path.join(IMAGES_ROOT_DIR, \"images_batch_*\"))\ntotal_images_processed = 0\n\nif len(batch_folders) == 0:\n    print(f\"\\nüö®üö® CRITICAL ERROR: '{IMAGES_ROOT_DIR}' mein koi 'images_batch_X' folder nahi mila. Path galat hai. üö®üö®\")\nelse:\n    print(f\"‚úÖ Kul {len(batch_folders)} image batches mile. Features nikalna shuru...\")\n\n    with torch.no_grad(): \n        for folder in tqdm(batch_folders, desc=\"Processing Batches\"):\n            image_files = glob.glob(os.path.join(folder, \"*.jpg\"))\n            \n            for image_path in image_files:\n                try:\n                    sample_id = int(os.path.splitext(os.path.basename(image_path))[0]) \n                    \n                    img = Image.open(image_path).convert('RGB')\n                    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n                    \n                    feature_vector = model(img_tensor).squeeze().cpu().numpy() \n                    image_embeddings[str(sample_id)] = feature_vector\n                    total_images_processed += 1\n                    \n                except Exception:\n                    continue\n\n    # Features ko .npz file mein save karein\n    np.savez_compressed(EMBEDDINGS_FILE, **image_embeddings) \n\n    # Memory saaf karein\n    del model, transform, image_embeddings\n    torch.cuda.empty_cache()\n\n    print(f\"\\n‚úÖ Image Processing Poora Hua!\")\n    print(f\"Kul images se features nikale gaye: {total_images_processed}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:10:31.333607Z","iopub.execute_input":"2025-10-13T14:10:31.334372Z","iopub.status.idle":"2025-10-13T15:34:42.176662Z","shell.execute_reply.started":"2025-10-13T14:10:31.334345Z","shell.execute_reply":"2025-10-13T15:34:42.175885Z"}},"outputs":[{"name":"stdout","text":"üöÄ Direct Image Loading aur Feature Extraction shuru...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 209MB/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Kul 16 image batches mile. Features nikalna shuru...\n","output_type":"stream"},{"name":"stderr","text":"Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [1:23:26<00:00, 312.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Image Processing Poora Hua!\nKul images se features nikale gaye: 79999\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Feature Fusion, Model Training, Aur Submission (FINAL WORKING VERSION)\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nimport os\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack\nfrom tqdm import tqdm\n\nprint(\"Step 3 shuru: Features ko joda ja raha hai aur model train kiya ja raha hai...\")\nprint(\"-\" * 50)\n\n# --- Configuration ---\nDATA_DIR = \"/kaggle/input/a-ds-ml/student_resource/dataset/\" \nEMBEDDINGS_FILE = \"/kaggle/working/image_embeddings.npz\"\nSUBMISSION_FILE = \"/kaggle/working/submission.csv\"\n\n# --- Data Load Karein ---\ntry:\n    train_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n    test_df = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n    image_embeddings_loaded = np.load(EMBEDDINGS_FILE, allow_pickle=True)\nexcept Exception as e:\n    print(f\"\\nüö® CRITICAL ERROR: Data Loading mein samasya. Exception: {e}\")\n    raise\n\n# --- 3.1: Text Feature Extraction (TF-IDF) ---\ntrain_text = train_df['catalog_content'].fillna('')\ntest_text = test_df['catalog_content'].fillna('')\n\ntfidf = TfidfVectorizer(stop_words='english', max_features=50000)\nX_train_text = tfidf.fit_transform(train_text)\nX_test_text = tfidf.transform(test_text)\n\nprint(f\"TF-IDF features ka shape: Train={X_train_text.shape}, Test={X_test_text.shape}\")\n\n# --- 3.2: Image Feature Integration ---\ndef get_image_features(df, embeddings_npz, feature_dim=2048):\n    features = []\n    default_feature = np.zeros(feature_dim, dtype=np.float32) \n    key_mapping = {int(k): v for k, v in embeddings_npz.items()}\n    \n    for sample_id in tqdm(df['sample_id'].values, desc=\"Image features ko prepare kiya ja raha hai\"):\n        feature = key_mapping.get(sample_id, default_feature)\n        features.append(feature)\n        \n    return np.array(features)\n\nX_train_img = get_image_features(train_df, image_embeddings_loaded)\nX_test_img = get_image_features(test_df, image_embeddings_loaded)\n\nimage_embeddings_loaded.close()\n\nprint(f\"Image features ka shape: Train={X_train_img.shape}, Test={X_test_img.shape}\")\n\n# --- 3.3: Feature Fusion (Text + Image) ---\nX_train = hstack([X_train_text, X_train_img])\nX_test = hstack([X_test_text, X_test_img])\ny_train = train_df['price']\n\n# üåü FINAL FIX: Sparse matrix ko CSR mein badlein taki slicing (indexing) ho sake\nX_train = X_train.tocsr() \n\nprint(f\"Final Combined Feature Matrix ka shape: Train={X_train.shape}, Test={X_test.shape}\")\n\n# --- 3.4: LightGBM Model Training (K-Fold Cross-Validation) ---\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\noof_preds = np.zeros(X_train.shape[0])\nsub_preds = np.zeros(X_test.shape[0])\n\nlgb_params = {\n    'objective': 'regression_l1',\n    'metric': 'mae',\n    'n_estimators': 3000,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 1,\n    'lambda_l1': 0.1,\n    'lambda_l2': 0.1,\n    'num_leaves': 31,\n    'verbose': -1,\n    'n_jobs': -1,\n    'seed': 42\n}\n\nprint(\"\\nModel Training shuru ho rahi hai (5 folds)...\")\n\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n    # Yeh line ab chal jaayegi kyunki X_train ab CSR format mein hai\n    X_train_fold, y_train_fold = X_train[train_idx], y_train[train_idx]\n    X_valid_fold, y_valid_fold = X_train[valid_idx], y_train[valid_idx]\n    \n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(X_train_fold, y_train_fold,\n                  eval_set=[(X_valid_fold, y_valid_fold)],\n                  eval_metric='mae',\n                  callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n\n    oof_preds[valid_idx] = lgb_model.predict(X_valid_fold)\n    sub_preds += lgb_model.predict(X_test) / folds.n_splits\n\nprint(f\"\\nModel MAE (Cross-Validation): {np.mean(np.abs(oof_preds - y_train)):.4f}\")\n\n# --- 3.5: Submission File Creation ---\nsubmission_df = pd.DataFrame({'ID': test_df['sample_id'], 'Price': sub_preds})\nsubmission_df['Price'] = np.maximum(0, submission_df['Price']) \nsubmission_df.to_csv(SUBMISSION_FILE, index=False)\n\nprint(f\"\\nüöÄ‚úÖ Step 3 Poora Hua! Final submission file '{SUBMISSION_FILE}' ban chuka hai. Congratulations!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:59:47.104117Z","iopub.execute_input":"2025-10-13T15:59:47.104766Z","iopub.status.idle":"2025-10-13T17:59:00.086287Z","shell.execute_reply.started":"2025-10-13T15:59:47.104740Z","shell.execute_reply":"2025-10-13T17:59:00.085419Z"}},"outputs":[{"name":"stdout","text":"Step 3 shuru: Features ko joda ja raha hai aur model train kiya ja raha hai...\n--------------------------------------------------\nTF-IDF features ka shape: Train=(75000, 50000), Test=(75000, 50000)\n","output_type":"stream"},{"name":"stderr","text":"Image features ko prepare kiya ja raha hai: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:00<00:00, 1741319.22it/s]\nImage features ko prepare kiya ja raha hai: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:00<00:00, 2880307.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Image features ka shape: Train=(75000, 2048), Test=(75000, 2048)\nFinal Combined Feature Matrix ka shape: Train=(75000, 52048), Test=(75000, 52048)\n\nModel Training shuru ho rahi hai (5 folds)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning(\"Converting data to scipy sparse matrix.\")\n/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning(\"Converting data to scipy sparse matrix.\")\n/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning(\"Converting data to scipy sparse matrix.\")\n/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning(\"Converting data to scipy sparse matrix.\")\n/usr/local/lib/python3.11/dist-packages/lightgbm/basic.py:1238: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning(\"Converting data to scipy sparse matrix.\")\n","output_type":"stream"},{"name":"stdout","text":"\nModel MAE (Cross-Validation): 11.7352\n\nüöÄ‚úÖ Step 3 Poora Hua! Final submission file '/kaggle/working/submission.csv' ban chuka hai. Congratulations!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"Train columns:\", train_df.columns.tolist())\nprint(\"Test columns:\", test_df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:51:47.972273Z","iopub.execute_input":"2025-10-13T15:51:47.972985Z","iopub.status.idle":"2025-10-13T15:51:47.976978Z","shell.execute_reply.started":"2025-10-13T15:51:47.972959Z","shell.execute_reply":"2025-10-13T15:51:47.976383Z"}},"outputs":[{"name":"stdout","text":"Train columns: ['sample_id', 'catalog_content', 'image_link', 'price']\nTest columns: ['sample_id', 'catalog_content', 'image_link']\n","output_type":"stream"}],"execution_count":10}]}